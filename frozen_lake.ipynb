{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -c anaconda ipython"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FrozenLake-v0\n",
    "\n",
    "### The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile.\n",
    "\n",
    "Winter is here. You and your friends were tossing around a frisbee at the park when you made a wild throw that left the frisbee out in the middle of the lake. The water is mostly frozen, but there are a few holes where the ice has melted. If you step into one of those holes, you'll fall into the freezing water. At this time, there's an international frisbee shortage, so it's absolutely imperative that you navigate across the lake and retrieve the disc. However, the ice is slippery, so you won't always move in the direction you intend.\n",
    "\n",
    "### The surface is described using a grid like the following:\n",
    "\n",
    "- SFFF       (S: starting point, safe)\n",
    "- FHFH       (F: frozen surface, safe)\n",
    "- FFFH       (H: hole, fall to your doom)\n",
    "- HFFG       (G: goal, where the frisbee is located)\n",
    "\n",
    "**The episode ends when you reach the goal or fall in a hole. You receive a reward of 1 if you reach the goal, and zero otherwise**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Environment variable for the game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating and Initialising the Q - Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]\n",
      " [0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "action_space = env.action_space.n\n",
    "state_space = env.observation_space.n\n",
    "\n",
    "\n",
    "q_table = np.zeros((state_space , action_space))\n",
    "print(q_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the parameters for our Algorithm's Learning and Execution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episode based constants\n",
    "EPISODES = 10000\n",
    "MAX_ITERATIONS_PER_EPISODE = 100\n",
    "\n",
    "# Learning and Reward based constants\n",
    "LEARNING_RATE_ALPHA = 0.1\n",
    "DISCOUNT_RATE_GAMMA = 0.99\n",
    "\n",
    "# Exploration and Exploitation based constants\n",
    "exploration_rate = 1 # Initial value of Epsilon\n",
    "MAX_EXPLORATION_RATE = 1  # Upper bound for exploration rate\n",
    "MIN_EXPLORATION_RATE = 0.01  # Lower bound for exploration rate\n",
    "EXPLORATION_DECAY_RATE = 0.001 # Decay per episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_from_episodes = [] # to hold the rewards we get from each episode and later observe how the agent learned\n",
    "\n",
    "# Algorithm \n",
    "\n",
    "for episode in range(EPISODES):\n",
    "    \n",
    "    state = env.reset() # The reset method returns us the current Environment state , from which we start playing\n",
    "    done = False # Setting a parameter ( like a flag ) to know if we have reached a terminal state or not\n",
    "    \n",
    "    rewards_current_episode = 0 # Initilaising the current episode reward\n",
    "    \n",
    "    for step in range(MAX_ITERATIONS_PER_EPISODE):\n",
    "        \n",
    "        # Exploration - Exploitation Trade-Off\n",
    "        \n",
    "        exploration_rate_threshold = random.uniform(0,1)\n",
    "        \n",
    "        if exploration_rate_threshold > exploration_rate:\n",
    "            action = np.argmax(q_table[state,:])\n",
    "        else:\n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        # Doing the Action\n",
    "        \n",
    "        new_state, reward , done , info = env.step(action)\n",
    "        \n",
    "        # Update the Q-Table for Q(s,a) based on the updation formula\n",
    "        # Q(s,a) = (1-alpha)*Q(s,a) + alpha(reward , max_reward(Q-table[state,:]))\n",
    "        \n",
    "        q_table[state,action] = q_table[state,action]*(1-LEARNING_RATE_ALPHA)+ LEARNING_RATE_ALPHA*(reward + DISCOUNT_RATE_GAMMA*np.max(q_table[new_state,:]))\n",
    "        \n",
    "        state = new_state\n",
    "        rewards_current_episode += reward\n",
    "        \n",
    "        if done == True:\n",
    "            break\n",
    "            \n",
    "            \n",
    "    # Exploration Rate Decay Procedure\n",
    "    \n",
    "    exploration_rate = MIN_EXPLORATION_RATE + (MAX_EXPLORATION_RATE - MIN_EXPLORATION_RATE)*np.exp(-EXPLORATION_DECAY_RATE*episode)\n",
    "    \n",
    "    \n",
    "    rewards_from_episodes.append(rewards_current_episode)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Average Rewards per Thousand Episodes **** \n",
      "\n",
      "1000  :  0.03800000000000003\n",
      "2000  :  0.21500000000000016\n",
      "3000  :  0.4200000000000003\n",
      "4000  :  0.5510000000000004\n",
      "5000  :  0.6140000000000004\n",
      "6000  :  0.6810000000000005\n",
      "7000  :  0.6630000000000005\n",
      "8000  :  0.6940000000000005\n",
      "9000  :  0.6870000000000005\n",
      "10000  :  0.6760000000000005\n",
      "\n",
      "\n",
      "********* Q-TABLE ********** \n",
      "\n",
      "[[0.51907373 0.50036143 0.51077019 0.50802164]\n",
      " [0.3547606  0.37401188 0.28186437 0.47418438]\n",
      " [0.40725072 0.42233127 0.40519175 0.45799795]\n",
      " [0.32003878 0.26844336 0.37940503 0.44630301]\n",
      " [0.55217645 0.28647335 0.36115532 0.45101651]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.30437663 0.15585007 0.210934   0.10671138]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.39321476 0.42068917 0.43279473 0.58359608]\n",
      " [0.45777763 0.62387816 0.41361401 0.29190728]\n",
      " [0.62844379 0.3945588  0.28023411 0.41651082]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.49097244 0.46564821 0.76390899 0.46918892]\n",
      " [0.70689993 0.8976457  0.77206293 0.74809454]\n",
      " [0.         0.         0.         0.        ]]\n"
     ]
    }
   ],
   "source": [
    "rewards_per_1000_episodes = np.split(np.array(rewards_from_episodes) , EPISODES/1000)\n",
    "count = 1000\n",
    "\n",
    "print(\"**** Average Rewards per Thousand Episodes **** \\n\")\n",
    "for r in rewards_per_1000_episodes:\n",
    "    print(count , ' : ' , str(sum(r/1000)))\n",
    "    count += 1000\n",
    "print('\\n')    \n",
    "    \n",
    "# Printing Updated Q-Value\n",
    "\n",
    "print('********* Q-TABLE ********** \\n')\n",
    "print(q_table)\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visulaising our Agent playing Frozen Lake :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (Down)\n",
      "SFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFF\u001b[41mG\u001b[0m\n",
      "**** You've reached the Goal !! ****\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "for episode in range(3):\n",
    "    \n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    \n",
    "    print(\"********** EPISODE : \",episode+1 , \"************\\n\\n\" )\n",
    "    time.sleep(2)\n",
    "    \n",
    "    for step in range(MAX_ITERATIONS_PER_EPISODE):\n",
    "        \n",
    "        clear_output(wait = True)\n",
    "        \n",
    "        env.render()\n",
    "        time.sleep(1)\n",
    "        \n",
    "        action = np.argmax(q_table[state,:])\n",
    "        new_state , reward , done , info = env.step(action)\n",
    "        \n",
    "        \n",
    "        if done:\n",
    "            clear_output(wait = True)\n",
    "            env.render()\n",
    "            \n",
    "            if reward == 1:\n",
    "                print(\"**** You've reached the Goal !! ****\")\n",
    "                time.sleep(2)\n",
    "            else:\n",
    "                print(\"**** You fell through a Hole !! ****\")\n",
    "                time.sleep(2)\n",
    "            \n",
    "            clear_output(wait = True)\n",
    "            break\n",
    "            \n",
    "            \n",
    "        state = new_state\n",
    "        \n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
